{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-46d843154dcd7d0f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# SLU10: Metrics for Classification -- Exercises\n",
    "\n",
    "In this notebook, we'll practice all the classification metrics you've learned. This is the roadmap:\n",
    "\n",
    "- Understanding the problem with accuracy\n",
    "- Understanding FP, TP, FN, TN\n",
    "- Understanding precision and recall\n",
    "- Understanding the ROC Curve \n",
    "- Understanding AUROC\n",
    "- Which model is better for a particular circumstance\n",
    "- Using these metrics in day to day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8d860e3a9483163",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math \n",
    "import numpy as np\n",
    "from utils import plot_confusion_matrix, hash_answer\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe9b148043106aa3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell - we will use this function in the exercises.\n",
    "\n",
    "def load_data():\n",
    "    # Loads classifier probabilities dataframe\n",
    "    df = pd.read_csv(\"data/classifier_prediction_scores.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9bd4090c02064258",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Useful information\n",
    "\n",
    "In this notebook, we will work with a dataset with probabilities and class labels like this:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>probas</th>      <th>target</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0.288467</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>0.255047</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>0.201017</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>0.729307</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>0.148288</td>      <td>0</td>    </tr>  </tbody></table>\n",
    "\n",
    "For some metrics, we will need to convert the probabilities into predicted class labels by setting a threshold. We will do it using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c181744c80d2ac0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell, we will use the function in the exercises.\n",
    "\n",
    "def threshold_probas(proba, threshold=.5): \n",
    "    if proba >= threshold:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8d34c4723b41a782",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1 - Understanding the problem with Accuracy \n",
    "\n",
    "In this exercise, you will implement a function to calculate the accuracy from true and predicted class labels. We will then compare the accuracy of predicted class labels for various predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f34a326a5fa50173",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement below a function to compute the accuracy from a dataframe with true and predicted class labels:\n",
    "\n",
    "- the `target` column contains the true labels\n",
    "- the `prediction` column contains the model prediction for the class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72ba6d5cbf2be8e0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(df):\n",
    "    \"\"\"\n",
    "        Given a dataframe with `prediction` and `target` columns, \n",
    "        compute the accuracy metric\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): the dataframe, with `prediction` and `target` columns\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): the accuracy metric    \n",
    "    \"\"\"    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-26325384c86158fa",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame({\"target\": np.array([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0])})\n",
    "\n",
    "df_1_all_zeros = df_1.copy()\n",
    "df_1_all_zeros[\"prediction\"] = 0 \n",
    "np.testing.assert_almost_equal(accuracy(df_1_all_zeros), 0.4545, 2)\n",
    "\n",
    "df_1_all_ones = df_1.copy()\n",
    "df_1_all_ones[\"prediction\"] = 1\n",
    "np.testing.assert_almost_equal(accuracy(df_1_all_ones), 0.54545, 2)\n",
    "\n",
    "df_2 = pd.DataFrame({\"target\": np.array([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])})\n",
    "\n",
    "df_2_all_zeros = df_2.copy()\n",
    "df_2_all_zeros[\"prediction\"] = 0 \n",
    "np.testing.assert_almost_equal(accuracy(df_2_all_zeros), 0.2727, 2)\n",
    "\n",
    "df_2_all_ones = df_2.copy()\n",
    "df_2_all_ones[\"prediction\"] = 1\n",
    "np.testing.assert_almost_equal(accuracy(df_2_all_ones), 0.7272, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-56fb3b7d3d008271",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's calculate the accuracy for predictions for a threshold = 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff217b96e0d0697f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# run this cell\n",
    "\n",
    "df = load_data()\n",
    "df[\"prediction\"] = df['probas'].apply(threshold_probas)\n",
    "print(f\"Accuracy of predictions: {accuracy(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b8c7341fba78da8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's see what would happen if we just predicted 0 (majority class) or 1 for every data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df9ea34d6d402024",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df[\"prediction\"] = 0\n",
    "print(f\"Accuracy of predicting always zero: {accuracy(df)}\")\n",
    "\n",
    "\n",
    "df[\"prediction\"] = 1\n",
    "print(f\"Accuracy of predicting always one: {accuracy(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67b13c4f54a6a31b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you see, the accuracy can be misleading. We got nearly the same accuracy for predicting all 0s and for our actual prediction with a threshold of 0.5 (0.84 vs. 0.85). If we optimized our model just for accuracy, we could end up with a model that just predicts all zero values. Let's move on to other metrics from the learning notebook.\n",
    "\n",
    "We'll start with the confusion matrix and the insights it gives us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d025a0824fb08ef3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2: Understanding metrics at a particular threshold \n",
    "\n",
    "We'll now look into confusion matrices. For this purpose we'll use a threshold of 0.3 and plot the confusion matrix. You will then use the numbers in the confusion matrix to calculate the true and false positives and negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f454e07e9cb07fa7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# run this cell to calculate predictions for threshold = 0.3 and plot the corresponding confusion matrix\n",
    "\n",
    "df = load_data()\n",
    "df[\"prediction\"] = df['probas'].apply(lambda prob: threshold_probas(prob, threshold=0.3))\n",
    "\n",
    "plot_confusion_matrix(df[\"target\"], df[\"prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6922da30d097e4be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1 FP, TP, FN, TN\n",
    "Identify the following parameters for this confusion matrix and fill in the answers in the cell below:\n",
    "\n",
    "- False Positives    (FP)\n",
    "- True Positives     (TP)\n",
    "- False Negatives    (FN)\n",
    "- True Negatives     (TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bfc98777980f60ed",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# true_positives\n",
    "# TP = ...\n",
    "# true_negatives \n",
    "# TN = ...\n",
    "# false_positives\n",
    "# FP = ...\n",
    "# false_negatives\n",
    "# FN = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f7dda939ca8ad6e4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert hash_answer(TP) == 'a46e37632fa6ca51a13fe39a567b3c23b28c2f47d8af6be9bd63e030e214ba38'\n",
    "assert hash_answer(TN) == '55f0124bb79f5c53d868ca45bbb0f4d04da15eea4fb29c6b95087fe8801bf0a3' \n",
    "assert hash_answer(FP) == '69f59c273b6e669ac32a6dd5e1b2cb63333d8b004f9696447aee2d422ce63763'\n",
    "assert hash_answer(FN) == '98010bd9270f9b100b6214a21754fd33bdc8d41b2bc9f9dd16ff54d3c34ffd71'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f6fc22e92299e8f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Great job! We'll now use these parameters to compute more metrics. Implement below a function for each of these metrics: \n",
    "\n",
    "- False Positive Rate  (FPR)\n",
    "- True Positive Rate (TPR)\n",
    "- Precision\n",
    "- Recall\n",
    "\n",
    "Note that all functions take the same arguments although some of them might be superfluous in each case.\n",
    "\n",
    "### 2.2 False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-516897dd70339b47",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def false_positive_rate(TP, TN, FP, FN):\n",
    "    \"\"\"\n",
    "    Given the true positives, true negatives, false positives, and false negatives\n",
    "    compute the false positive rate\n",
    "    \n",
    "    Args:\n",
    "        TP (int): number of true positives     \n",
    "        TN (int): number of true negatives     \n",
    "        FP (int): number of false positives     \n",
    "        FN (int): number of false positives     \n",
    "\n",
    "\n",
    "    Returns:\n",
    "        false_positive_rate (float): false positive rate     \n",
    "    \"\"\"    \n",
    "\n",
    "    # false_positive_rate = ...  (a.k.a False Alarm Rate)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return false_positive_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d472dab689d7cb50",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(false_positive_rate(10, 40, 13, 42), 0.2452, 2)\n",
    "np.testing.assert_almost_equal(false_positive_rate(24, 42, 4, 64), 0.0869, 2)\n",
    "np.testing.assert_almost_equal(false_positive_rate(64, 26, 64, 15), 0.7111, 2)\n",
    "np.testing.assert_almost_equal(false_positive_rate(17, 15, 1, 3), 0.0625, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3b34bc1323ecb94f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3 True Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a740e1c5ee69421a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def true_positive_rate(TP, TN, FP, FN):\n",
    "    \"\"\"\n",
    "    Given the true positives, true negatives, false positives, and false negatives\n",
    "    compute the true positive rate\n",
    "    \n",
    "    Args:\n",
    "        TP (int): number of true positives     \n",
    "        TN (int): number of true negatives     \n",
    "        FP (int): number of false positives     \n",
    "        FN (int): number of false positives     \n",
    "\n",
    "\n",
    "    Returns:\n",
    "        true_positive_rate (float): true positive rate     \n",
    "    \"\"\"    \n",
    "\n",
    "    # true_positive_rate = ... \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return true_positive_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-caa8049e86187fa1",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(true_positive_rate(10, 40, 13, 42), 0.1923, 2)\n",
    "np.testing.assert_almost_equal(true_positive_rate(24, 42, 4, 64), 0.2727, 2)\n",
    "np.testing.assert_almost_equal(true_positive_rate(64, 26, 64, 15), 0.8101, 2)\n",
    "np.testing.assert_almost_equal(true_positive_rate(17, 15, 1, 3), 0.8500, 2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-810deeec6e16fe7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.4 Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c66a7234a25f7dc8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def precision_metric(TP, TN, FP, FN):\n",
    "    \"\"\"\n",
    "    Given the true positives, true negatives, false positives, and falsenegatives\n",
    "    compute the precision\n",
    "    \n",
    "    Args:\n",
    "        TP (int): number of true positives     \n",
    "        TN (int): number of true negatives     \n",
    "        FP (int): number of false positives     \n",
    "        FN (int): number of false positives     \n",
    "\n",
    "\n",
    "    Returns:\n",
    "        precision (float): precision score     \n",
    "    \"\"\"    \n",
    "    # precision = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a10585e0d5167a4d",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(precision_metric(10, 40, 13, 42), 0.4348, 2)\n",
    "np.testing.assert_almost_equal(precision_metric(24, 42, 4, 64), 0.8571, 2)\n",
    "np.testing.assert_almost_equal(precision_metric(64, 26, 64, 15), 0.5000, 2)\n",
    "np.testing.assert_almost_equal(precision_metric(17, 15, 1, 3), 0.94444, 2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0ae2181566de687",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.5 Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4cf0fcc674b2d58f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def recall_metric(TP, TN, FP, FN):\n",
    "    \"\"\"\n",
    "    Given the true positives, true negatives, false positives, and false negatives\n",
    "    compute the recall\n",
    "    \n",
    "    Args:\n",
    "        TP (int): number of true positives     \n",
    "        TN (int): number of true negatives     \n",
    "        FP (int): number of false positives     \n",
    "        FN (int): number of false positives     \n",
    "\n",
    "\n",
    "    Returns:\n",
    "        recall (float): recall score     \n",
    "    \"\"\"    \n",
    "    # recall = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-51b114e42c819d0f",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(recall_metric(10, 40, 13, 42), 0.1923, 2)\n",
    "np.testing.assert_almost_equal(recall_metric(24, 42, 4, 64), 0.2727, 2)\n",
    "np.testing.assert_almost_equal(recall_metric(64, 26, 64, 15), 0.8101, 2)\n",
    "np.testing.assert_almost_equal(recall_metric(17, 15, 1, 3), 0.8500, 2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b3bed38e3cf2ee1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, let's apply our metrics to the dataframe we had. We'll compute these for the threshold of 0.3, which corresponds to the confusion matrix presented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ed0312a1da01fce",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "FPR = false_positive_rate(TP, TN, FP, FN)\n",
    "TPR = true_positive_rate(TP, TN, FP, FN)\n",
    "precision = precision_metric(TP, TN, FP, FN)\n",
    "recall = recall_metric(TP, TN, FP, FN)\n",
    "\n",
    "print('True Positives:       %0.0f' % TP)\n",
    "print('True Negatives:       %0.0f' % TN)\n",
    "print('False Positives:      %0.0f' % FP)\n",
    "print('False Negatives:      %0.0f' % FN)\n",
    "print('False Positive Rate:  %0.2f' % FPR)\n",
    "print('True Positive Rate:   %0.2f' % TPR)\n",
    "print('precision:            %0.2f' % precision)\n",
    "print('recall:               %0.2f' % recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f1a3d33376c70afb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "_Note: your `True Positive Rate` and `Recall` should be the same because... ehm... check their formula!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2e542b0c2502309e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 3: model selection \n",
    "\n",
    "Consider the following two confusion matrixes: \n",
    "\n",
    "<img src=\"media/conf_mats.png\" alt=\"drawing\" width=\"600\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc304cc682a1bb1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Which model (A or B) is better if they represent the performance of a judge over their career? \n",
    "In this situation: \n",
    "- 1 is guilty\n",
    "- 0 is innocent \n",
    "- predicting 1 sends people to jail\n",
    "- predicting 0 sends people home free\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b50fa7cfe6ed2f83",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# best_option_for_a_judge = ... (\"A\" or \"B\")\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6147950c9874737e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Which option (A or B) is best, if this is a model for cancer screening? \n",
    "In this situation: \n",
    "- 1 is cancer\n",
    "- 0 is healthy\n",
    "- predicting 1 sends people to a cancer screening\n",
    "- predicting 0 sends people home "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67d2c8b099960bb4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# best_option_for_cancer_screening = ... (\"A\" or \"B\")\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4f6e108311857c10",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert hash_answer(best_option_for_cancer_screening.lower()) == '3e23e8160039594a33894f6564e1b1348bbd7a0088d42c4acb73eeaed59c009d' \n",
    "assert hash_answer(best_option_for_a_judge.lower()) == 'ca978112ca1bbdcafac231b39a23dc4da786eff8147c4e72b9807785afee48bb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0fbe84422a1518aa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 4: Understanding the ROC Curve \n",
    "\n",
    "In this exercise, you will calculate the ROC curve step-by-step. You get the first two functions for free and you need to implement the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c0e4693c249ada78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# functions for free\n",
    "\n",
    "def threshold_probas(proba, threshold): \n",
    "    \"\"\"\n",
    "    Returns the prediction for a single observation based on a threshold\n",
    "    \n",
    "    Args:\n",
    "        proba (float): a scores or probability, between 0.0 and 1.0\n",
    "        threshold (float): a number between 0.0 and 1.0    \n",
    "    \n",
    "    Returns:\n",
    "        the prediction: 1 or 0\n",
    "    \"\"\" \n",
    "    if proba >= threshold:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0 \n",
    "    \n",
    "def get_predictions(probas, threshold):\n",
    "    \"\"\"\n",
    "    Returns the predictions for a series of observations\n",
    "    \n",
    "    Args:\n",
    "        probas (pd.Series): a series of floats (scores or probabilities) between 0.0 and 1.0\n",
    "        threshold (float): a number between 0.0 and 1.0    \n",
    "    Returns:\n",
    "        predictions (pd.Series): a series of 1s and 0s \n",
    "    \"\"\"\n",
    "    return probas.apply(lambda x: threshold_probas(x, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba759d9d8e179316",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.1. Confusion Matrix Elements \n",
    "A second take on the confusion matrix. Write a function that calculates all elements of the Confusion Matrix (TP, FP, TN, FN) from a dataset of true and predicted class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0a058e0a8e2834d4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(predictions, target):\n",
    "    \"\"\"\n",
    "    Calculates all the elements of the confusion matrix\n",
    "    \n",
    "    Args:\n",
    "        predictions (pd.Series): a series of ints, either 0s or 1s, with predictions\n",
    "        target (pd.Series): a series of ints, either 0s or 1s, with observed outcomes \n",
    "    \n",
    "    Returns:\n",
    "        true_positives, true_negatives, false_positives, false_negatives (int):\n",
    "                                                    elements of the confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # it should take ~2 lines to calculate each measure\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return true_positives, true_negatives, false_positives, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2a5fde6327485279",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "threshold_for_testing = 0.6\n",
    "predictions = get_predictions(df['probas'], threshold_for_testing)\n",
    "tp, tn, fp, fn = compute_confusion_matrix(predictions, df.target)\n",
    "\n",
    "assert hash_answer(tp) == '4a44dc15364204a80fe80e9039455cc1608281820fe2b24f1e5233ade6af1dd5'\n",
    "assert hash_answer(fp) == 'ef2d127de37b942baad06145e54b0c619a1f22327b2ebbcfbec78f5564afe39d'\n",
    "assert hash_answer(tn) == 'd6723fa996ced47773f2dea29cce9b11f951e6dafe321a84ac7d32791c3b4660'\n",
    "assert hash_answer(fn) == '2abaca4911e68fa9bfbf3482ee797fd5b9045b841fdff7253557c5fe15de6477'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1b3c9ba7eda8923",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.2. True/False Positives Rates\n",
    "Write a function that calculates the True Positives Rate and the False Positives Rate from the true and predicted class labels. You can use the functions you have implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-876416c5941a720c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_true_and_false_positives_rate(predictions, target):\n",
    "    \"\"\"\n",
    "    Calculates the true and false positives rate\n",
    "    \n",
    "    Args:\n",
    "        predictions (pd.Series): a series of ints, either 0s or 1s, with predictions\n",
    "        target (pd.Series): a series of ints, either 0s or 1s, with observed outcomes \n",
    "    \n",
    "    Returns:\n",
    "        true_positives_rate, false_positives_rate (float): true and false positives rate\n",
    "    \"\"\"   \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return true_positives_rate, false_positives_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c4517396cfce00b0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "threshold_for_testing = 0.6\n",
    "predictions = get_predictions(df['probas'], threshold_for_testing)\n",
    "tpr, fpr = calculate_true_and_false_positives_rate(predictions, df['target'])\n",
    "\n",
    "assert hash_answer(round(tpr, 4)) == '918ae0d325e428351045f50747561dd25451e3866462a38c0e62ec2913fdd1a5'\n",
    "assert hash_answer(round(fpr, 4)) == '4c158422ed9316829516c182728418409140930df77cf52b47ea03cb59bfd2a3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d9420d7580801e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.3. ROC Curve\n",
    "Now you have all the functions that help you to calculate the ROC curve. Implement a function that calculates the true and false positives rates for a list of thresholds. The function should return two dictionaries, for the true/false positives rate, where the keys are the thresholds and the values are the calculated true/false positives rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-845d59b49ea2fe9f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def hand_made_roc_curve(probas, target, list_of_thresholds):\n",
    "    \"\"\"\n",
    "    Calculates the true and false positives rates for the given thresholds.\n",
    "    \n",
    "    Args:\n",
    "        probas (float): a list of probabilities for some observations, between 0.0 and 1.0\n",
    "        target (int): a list of true class labels for the same observations, 0s and 1s\n",
    "        list_of_thresholds (float): a list of thresholds for which to calculate the true\n",
    "                                    and false positive rate\n",
    "    Returns:\n",
    "        TPRs, FPRSs: dictionaries with true and false positives rates for each threshold,\n",
    "                     keys - thresholds, values - corresponding tpr/fpr\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    TPRs = {}\n",
    "    FPRs = {}\n",
    "    \n",
    "    # for each threshold, get the predictions and calculate the TPR and FPR\n",
    "    # you already defined everything above, so each line should be just a function call   \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return TPRs, FPRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5d25008fa21ed0ab",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "list_of_thresholds = np.linspace(0, 10) ** 2 / 100\n",
    "\n",
    "hand_made_tpr, hand_made_fpr = hand_made_roc_curve(df['probas'], df['target'], list_of_thresholds)\n",
    "\n",
    "assert isinstance(hand_made_tpr, dict), \"TPRs is not a dictionary\"\n",
    "assert isinstance(hand_made_fpr, dict), \"FPRs is not a dictionary\"\n",
    "\n",
    "assert sum(list(hand_made_tpr.keys())== list_of_thresholds) == len(list_of_thresholds), 'The keys of the TPR dictionary are not correct'\n",
    "assert sum(list(hand_made_fpr.keys())== list_of_thresholds) == len(list_of_thresholds), 'The keys of the FPR dictionary are not correct'\n",
    "\n",
    "tpr_array = np.array(list(hand_made_tpr.values()))\n",
    "fpr_array = np.array(list(hand_made_fpr.values()))\n",
    "\n",
    "np.testing.assert_almost_equal(tpr_array.mean(), 0.56, 2, err_msg='The values of the TPR dictionary are not correct.')\n",
    "np.testing.assert_almost_equal(tpr_array.var(), 0.14, 2, err_msg='The values of the TPR dictionary are not correct.')\n",
    "np.testing.assert_almost_equal(tpr_array.min(), 0., 2, err_msg='The values of the TPR dictionary are not correct.')\n",
    "np.testing.assert_almost_equal(tpr_array.max(), 1., 2, err_msg='The values of the TPR dictionary are not correct.')\n",
    "\n",
    "np.testing.assert_almost_equal(fpr_array.mean(), 0.29, 2, err_msg='The values of the FPR dictionary are not correct.')\n",
    "np.testing.assert_almost_equal(fpr_array.var(), 0.10, 2, err_msg='The values of the FPR dictionary are not correct.')\n",
    "np.testing.assert_almost_equal(fpr_array.min(), 0.0, 1, err_msg='The values of the FPR dictionary are not correct.')\n",
    "np.testing.assert_almost_equal(fpr_array.max(), 1.0, 1, err_msg='The values of the FPR dictionary are not correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-32d989867a472234",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, the fruits of your labor. We will plot an ROC curve calculated with your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5b547305337b81d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "\n",
    "# generating a space of thresholds (we'll be more \"dense\" near zero)\n",
    "list_of_thresholds = np.linspace(0, 10) ** 2 / 100\n",
    "\n",
    "# using the function you have just created \n",
    "hand_made_tpr, hand_made_fpr = hand_made_roc_curve(df['probas'], df['target'], list_of_thresholds)\n",
    "\n",
    "# just making a dataframe to pretty things up\n",
    "hand_made_roc_df = pd.DataFrame({'False Positive Rate': hand_made_fpr, \n",
    "                        'True Positive Rate': hand_made_tpr})\n",
    "\n",
    "# naming the index\n",
    "hand_made_roc_df.index.name = 'Thresholds'\n",
    "\n",
    "# plotting the ROC curve \n",
    "hand_made_roc_df.set_index('False Positive Rate').plot();\n",
    "plt.show()\n",
    "\n",
    "# displaying the ROC curve data\n",
    "display(hand_made_roc_df.iloc[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3422c5927ff8fbf9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 5: The joy of scikit \n",
    "Ok, you earned it - you get to use scikit now ! :)\n",
    "\n",
    "Run the cell below to load the data and create predicted class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-525a214e5b698b83",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "df['prediction'] = get_predictions(df['probas'], threshold=.3)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-17e5387e40bc1bfb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5.1. Scikit Accuracy/Precision/Recall\n",
    "Implement a function to calculate the accuracy, precision, and recall with sklearn. Don't forget to import the necessary functions! And don't confuse the inputs. :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67c2e8ef543f95ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_precision_recall_sklearn(predictions, target):\n",
    "    \"\"\"\n",
    "    Given the predicted and true labels, return the accuracy, precision,\n",
    "    and recall using sklearn\n",
    "    \n",
    "    Args:\n",
    "        predictions (pd.Series or np.array): classifier class predictions\n",
    "        target (pd.Series or np.array): true labels     \n",
    "\n",
    "    Returns:\n",
    "        accuracy, precision, recall (float): metrics calculated with sklearn\n",
    "    \"\"\"    \n",
    "    # NOTE: Even though this is not a good programming practice, just \n",
    "    #       for the sake of the exercise, ensure you import the right \n",
    "    #       functions within this code:\n",
    "    # from ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-78d7b23cb6126162",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "df['prediction'] = get_predictions(df['probas'], threshold=.3)\n",
    "\n",
    "df_1 = df.iloc[100:200].copy()\n",
    "accuracy, precision, recall = accuracy_precision_recall_sklearn(df_1['prediction'], df_1['target'])\n",
    "np.testing.assert_almost_equal(accuracy, 0.83, 2, err_msg=\"The accuracy is not correct.\")\n",
    "np.testing.assert_almost_equal(precision, 0.5714, 2, err_msg=\"The precision is not correct.\")\n",
    "np.testing.assert_almost_equal(recall, 0.7619, 2, err_msg=\"The recall is not correct.\")\n",
    "\n",
    "df_2 = df.iloc[0:50].copy()\n",
    "accuracy, precision, recall = accuracy_precision_recall_sklearn(df_2['prediction'], df_2['target'])\n",
    "np.testing.assert_almost_equal(accuracy, 0.86, 2, err_msg=\"The accuracy is not correct.\")\n",
    "np.testing.assert_almost_equal(precision, 0.5, 2, err_msg=\"The precision is not correct.\")\n",
    "np.testing.assert_almost_equal(recall, 0.5714, 2, err_msg=\"The recall is not correct.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7233a3932138422b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's check the metrics for the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a3e823482a3aab0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "df['prediction'] = get_predictions(df['probas'], threshold=.3)\n",
    "accuracy, precision, recall = accuracy_precision_recall_sklearn(df['prediction'], df['target'])\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dac11876dbeaeb49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notice the great accuracy (which, as we know, is just trying to delude us), but not so great precision and OKish recall.\n",
    "\n",
    "Next level - confusion matrix with sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d200c1e87f5b716",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5.2. Scikit Confusion Matrix\n",
    "Implement a function that return the confusion matrix for the given true and predicted class labels. Use sklearn and don't forget to import. Don't confuse the function inputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86d92e523ba4389c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_sklearn(predictions, target):\n",
    "    \"\"\"\n",
    "    Given the predicted and true labels, return the confusion\n",
    "    matrix using scikit-learn\n",
    "    \n",
    "    Args:\n",
    "        predictions (pd.Series or np.array): classifier predictions     \n",
    "        target (pd.Series or np.array): true labels     \n",
    "\n",
    "    Returns:\n",
    "        conf_mat: confusion_matrix\n",
    "    \"\"\"    \n",
    "    # from ? import ...\n",
    "    # conf_mat = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4e5021ddb84188d6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "df['prediction'] = get_predictions(df['probas'], threshold=.3)\n",
    "\n",
    "df_1 = df.iloc[100:200].copy()\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    confusion_matrix_sklearn(df_1['prediction'], df_1['target']), \n",
    "    [[67, 12], [5,  16]], \n",
    "    2\n",
    ")\n",
    "\n",
    "df_2 = df.iloc[0:50].copy()\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    confusion_matrix_sklearn(df_2['prediction'], df_2['target']), \n",
    "    [[39, 4], [3, 4]], \n",
    "    2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9f560d037ce9b5f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And we can check the matrix for the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe94dbd2ee56d159",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "df['prediction'] = get_predictions(df['probas'], threshold=.3)\n",
    "\n",
    "print(f\"Confusion matrix:\\n {confusion_matrix_sklearn(df['prediction'], df['target'])}\")\n",
    "print(\"Meaning:\\n [[TP   FN]\\n [ FP  TN]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9d834085b457a15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We're pretty good in identifying positives, but quite bad in detecting negatives.\n",
    "\n",
    "### 5.3. Scikit ROC Curve\n",
    "Write a function which calculates the ROC curve and AUROC score using sklearn. The function should return the list of false positives rates, true positives rates, and the thresholds, and the AUROC score. As usual, don't forget the imports and don't confuse the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a1d01de0d2450f8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_roc_sklearn(probas, target):\n",
    "    \"\"\"\n",
    "    Given the predicted probabilities (probas) and true labels, \n",
    "    return the information regarding the roc curve - false positives rate,\n",
    "    true positives rate and thresholds - and the area under the curve\n",
    "    \n",
    "    Args:\n",
    "        probas (pd.Series or np.array): classifier probabilities     \n",
    "        target (pd.Series or np.array): true labels     \n",
    "\n",
    "    Returns:\n",
    "        fpr: false positives rates array for roc curve\n",
    "        tpr: true positives rates array for roc curve\n",
    "        thresholds: thresholds array for roc curve\n",
    "        roc_auc: roc area under the curve\n",
    "\n",
    "    \"\"\"    \n",
    "\n",
    "    # from ? import ... \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return fpr, tpr, thresholds, roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-74e565ebc3dd0dc9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "df['prediction'] = get_predictions(df['probas'], threshold=.3)\n",
    "\n",
    "df_1 = df.iloc[100:200].copy()\n",
    "fpr_1, tpr_1, thresholds_1, roc_auc_1 = get_roc_sklearn(df_1['probas'], df_1['target'])\n",
    "\n",
    "np.testing.assert_array_almost_equal(fpr_1[3:6], [0.013, 0.013, 0.025], 3)\n",
    "np.testing.assert_array_almost_equal(tpr_1[6:8], [0.333, 0.333], 3)\n",
    "np.testing.assert_array_almost_equal(thresholds_1[7:10], [0.502, 0.498, 0.477], 3)\n",
    "np.testing.assert_almost_equal(roc_auc_1, 0.86197, 2)\n",
    "\n",
    "df_2 = df.iloc[0:50].copy()\n",
    "fpr_2, tpr_2, thresholds_2, roc_auc_2 = get_roc_sklearn(df_2['probas'], df_2['target'])\n",
    "\n",
    "np.testing.assert_array_almost_equal(fpr_2[3:6], [0.047, 0.07 , 0.07], 3)\n",
    "np.testing.assert_array_almost_equal(tpr_2[1:3], [0.143, 0.143], 3)\n",
    "np.testing.assert_array_almost_equal(thresholds_2[7:10], [0.052, 0.014, 0.011], 3)\n",
    "np.testing.assert_almost_equal(roc_auc_2, 0.7575, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a976651c2a94bc79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, let's run it for the whole dataset and plot the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f9752ffe16d8a1b6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "df['prediction'] = get_predictions(df['probas'], threshold=.3)\n",
    "\n",
    "fpr_test, tpr_test, thresholds_test, roc_auc_test = get_roc_sklearn(df['probas'], df['target'])\n",
    "plt.figure(figsize=(4,4))\n",
    "lw = 2\n",
    "plt.plot(fpr_test, tpr_test, color='orange', lw=lw, label='ROC curve (AUROC = %0.2f)' % roc_auc_test)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', label='random')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a54695f047638506",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Congratulations! You made it to the end of another unit! You are now ready to explore more about classification and regression problems and other particularities of model training and selection.\n",
    "\n",
    "See you in the next unit!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
