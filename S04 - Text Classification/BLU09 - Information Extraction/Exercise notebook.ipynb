{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a3dc8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f881c7002cca9b48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# BLU09 - Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125f899",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4efa4dd0f5a58872",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# importing needed packages here\n",
    "\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from utils import remove_punctuation, remove_stopwords\n",
    "\n",
    "def _hash(s):\n",
    "    return hashlib.sha256(json.dumps(str(s)).encode()).hexdigest()\n",
    "\n",
    "cpu_count = int(os.cpu_count()) if os.cpu_count() != None else 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9f847",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a882aedc21b3fc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this exercise notebook you are going to tackle a quite real problem: **Detecting fake news!** Let's create a binary classifier to determine if a piece of news is considered 'reliable' or 'unreliable'. You will start by building some basic features, then go on to build more complex ones, and finally put it all together. You should be able to have a working classifier by the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255edbe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b947ffb2b30ff870",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset we will be using is the [Fake News](https://www.kaggle.com/c/fake-news/overview) from Kaggle. Each piece of news is either reliable or trustworthy, '0', or unreliable and possibly fake, '1'. First, let's load it and see what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4d9f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62fce116d684ff08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"datasets/fakenews/train.csv\"\n",
    "df = pd.read_csv(data_path, index_col=0)\n",
    "df[\"title\"] = df[\"title\"].astype(str)\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "df = df[:5000]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40814b45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38f5eb775841d955",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can see that we have 4 columns that are pretty self-explanatory. Let's drop the author column since we only want to practice our text analysis and drop the title as well for simplicity sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2118e6f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4555d789c2c7417",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=[\"author\", \"title\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ec050",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-043e168c017b27d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's also load SpaCy's module with the merged entities (which will come in handy later) and stopwords. `merge_entities` is this function https://spacy.io/api/pipeline-functions#merge_entities. We insert it into the SpaCy pipeline after the NER module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a289d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb489232f7a726b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe(\"merge_entities\", after=\"ner\")\n",
    "en_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d931a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed6699286cf9b2f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here we process the news text with SpaCy. This might take a while depending on your hardware (a break to walk the dog? ðŸ¶)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313691e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88f6782b20750823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "docs = list(tqdm(nlp.pipe(df[\"text\"], batch_size=20, n_process=cpu_count-1), total=len(df[\"text\"])))\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f722c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-276cdb3161f052ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Overall, the text looks good! Not too many errors, well written... as expected from a news article. Fake news is a very tough, recent problem that is now appearing more and more frequently in the wild. Usually there aren't many ortographic mistakes or slang (as it may happen with spam) since it's coming from news sources that want to appear credible but also clickbaity so that they can profit on that good ad revenue and create distrust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c38a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f9871459d26c91a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1. Pipeline\n",
    "\n",
    "With our text processed, let's get a baseline model for our classification problem! Let's use our comfortable _TfidfVectorizer_ to get a simple, fast and trustworthy baseline.\n",
    "\n",
    "Create a function that applies a pipeline to the given train data, makes a prediction for the test data, and returns the accuracy of the prediction. The pipeline should consist of a *TfidfVectorizer* and a *RandomForestClassifier* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ff301",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1cf7d1d751604527",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tfidf_rf_pipeline(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Trains a TfidfVectorizer + RandomForestClassifier pipeline for the given train data.\n",
    "    Makes a prediction.\n",
    "    Returns the trained pipeline and the accuracy of the prediction.\n",
    "    X_train, y_train: train data, pd.Series\n",
    "    X_test, y_test: test data, pd.Series\n",
    "    \"\"\"\n",
    "    \n",
    "    # pipe = (...)\n",
    "    # pipe.fit(...)\n",
    "    # (...)\n",
    "    # acc =\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return pipe, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac3eb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-31eb2fa6359bdecb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the baseline, we will preprocess the text - remove punctuaction and stopwords and tokenize it - then run it through the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59999879",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c345c9379c620ae2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_processed = df.copy()\n",
    "df_processed[\"text\"] = df_processed[\"text\"].apply(remove_punctuation)\n",
    "df_processed[\"text\"] = df_processed[\"text\"].apply(remove_stopwords, stopwords = en_stopwords, \n",
    "                                                  tokenizer = WordPunctTokenizer())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_processed[\"text\"], df_processed[\"label\"], \n",
    "                                                    test_size=0.2, random_state=42, stratify=df_processed[\"label\"])\n",
    "baseline_model, baseline_acc = tfidf_rf_pipeline(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# asserts\n",
    "assert isinstance(baseline_model, Pipeline)\n",
    "assert _hash(baseline_model[0]) == 'e68c8e581c16f0d62f3b9cb33a7967b17890e18c1fe819d013181e6714e7a303',\"The\\\n",
    "pipeline parameters are not correct.\"\n",
    "assert _hash(baseline_model[1]) == 'e5fd22909dcc06f7c81407ee302879e41a75675ddfd55fa1ec640ae68a3338d8',\"The\\\n",
    "pipeline parameters are not correct.\"\n",
    "assert np.allclose(baseline_acc, 0.908, 0.1), \"something wrong with the accuracy score. Use the default parameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1ec49",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ecc682a6b13cabf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Wow, the accuracy is quite good for such a simple text model! This just proves that a starting trustworthy baseline is all you need. I can't stress enough that it's really important to have a simple first iteration, and afterwards we can add complexity and study which features make sense or not, testing more out of the box solutions. \n",
    "\n",
    "Sometimes, data scientists focus right off the bat on the most complex solutions and a simple one would be enough. Real life problems will obviously achieve lower scores as the datasets are not controlled or cleaned for you but that should not stop you from starting with a simpler and easier solution.\n",
    "\n",
    "Now let's see if we can engineer other features. We will extract information with SpaCy and use it to train the same pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3f611",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9acda58125c99fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q2. SpaCy Matcher\n",
    "\n",
    "Let's see if we can extract some useful features by using our SpaCy Matcher.\n",
    "\n",
    "#### Q2.a) Simple Matcher\n",
    "\n",
    "You think of some words that could be related with the detection of Fake News. Something starts ringing in your mind about \"propaganda\", \"USA\" and \"fraud\", so you decide to check how many of those words appear in our news articles using the SpaCy Matcher.\n",
    "\n",
    "Use the docs list preprocessed by SpaCy and count the number of occurences of these words in all `Doc`s. The output should be the sum of occurencies in all news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f158c1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca5f9a618bc32ee6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "words = [\"propaganda\", \"USA\", \"fraud\"]\n",
    "\n",
    "# init the matcher - remember it from the learning notebook\n",
    "# add the patterns of the words. HINT: for a direct match you need a specific pattern (check SpaCy documentation)\n",
    "# count how many matches in all news articles\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# count = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6002d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-57893459c2e5a1ac",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert _hash(count) == '2357bc4ef05103860befa4a49fd8bbaa1541640845f4b89c1733bb4d6eb7cfcf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5374a2f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9a8e7ce0cab87fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.b) POS-Tagging Search\n",
    "\n",
    "Ok, this doesn't look like the way to go, let's look at other theories. You start thinking that fake news might exaggerate on adjectives and adverbs by using exaggerated or over the top descriptions. So you decide to create a feature that counts the number of _Adjectives_ and _Adverbs_ in a piece of news article. The count should be normalized to the word count of the article. Don't forget to exclude the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170ff4c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d048ae4dde4a43cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# HINT: you already have your news text processed (the docs variable),\n",
    "# so you can go over every doc and check if there is any POS Tag which is an ADJ or ADV\n",
    "# to check the POS tag of a token in a doc -----> token.pos_\n",
    "\n",
    "\"\"\"\n",
    "Try it out by running the below code! \n",
    "for token in docs[0]:\n",
    "    print(token.pos_)\n",
    "\"\"\"\n",
    "\n",
    "# Return a list with the number of adjectives and adverbs for every piece of news in docs\n",
    "# Normalize it to the number of words in the given article\n",
    "# nb_adj_adv = [...]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618744a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3961c5c5cb9b18aa",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(nb_adj_adv) == list, \"The result should be a list with just 1 dimension.\"\n",
    "assert len(nb_adj_adv) == 5000, \"The length of the result list is wrong.\\\n",
    "You should have a count for every news article.\"\n",
    "np.testing.assert_almost_equal(np.mean(nb_adj_adv), 0.1, decimal=1, err_msg='The result is not correct.')\n",
    "np.testing.assert_almost_equal(np.sum(nb_adj_adv), 528, decimal=0, err_msg='The result is not correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be4f20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fa92a0511c909523",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's add this feature to our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa3004",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-820cb2d992a833a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_processed[\"nb_adj_adv\"] = nb_adj_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74452de0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e54363f2d61151f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.c) Adjectivized proper nouns\n",
    "\n",
    "Another theory that might be worth testing is that adjectives with proper nouns are often used in this kind of news to induce sentiments towards people or organization. You want to extract proper nouns preceeded by adjectives to maybe use in a later analysis.\n",
    "\n",
    "Create a `Matcher` to search for adjective + proper noun. Count the number of occurences of each and output the 10 most common as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fe123",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a5547e1de72b597",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# I'll reset the matcher for you\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# pattern = [...] to find adjectives followed by proper nouns\n",
    "# matcher.add(\"\", pattern)\n",
    "\n",
    "# for doc in docs:\n",
    "# do matches and save the text in a list\n",
    "\n",
    "# count the number of times the same expression appears in the list (hint: remember the dictionary solution...)\n",
    "# take the top 10 of the counter\n",
    "# the result will be a list of tuples of the form (count, expression)\n",
    "\n",
    "# most_common_adj_propn = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de656a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a55276f7a5cb1f58",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(most_common_adj_propn) == list, \"the output is not a list\"\n",
    "assert len(most_common_adj_propn) == 10, \"It should be the top 10!\"\n",
    "\n",
    "assert _hash(most_common_adj_propn) == '0b6595146c52b5f1ce86ed96d3390418babe3c5da1497e2c97ea97cf1a830387', 'The top ten list is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79678441",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5de1f6e3a47a8db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's look at the 10 most common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b06dc3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a03eaf795b36ce50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_common_adj_propn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7e98e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7ba7a85acd3ab24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The counts seem to be too low to use these terms as features. Maybe running a vectorizer on all the results could work better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f299d11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-13de41c333217acd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.d) Objects of preposition\n",
    "Objects of the sentences could indicate something. For instance, 'NGO financed by Soros' is more likely to appear in fake news than 'NGO financed by UNESCO'. Both objects in these sentences are objects of preposition (hint: SpaCy has a dependency label for this).\n",
    "\n",
    "Create a `Matcher` to search for objects of preposition which are nouns. Again, count the number of occurences of each and output the 10 most common as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea459a6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d047424aca69651a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# I'll reset the matcher for you\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# pattern = [...] to find objects of preposition that are nouns\n",
    "# hint: you need to use dependency and POS labels\n",
    "# matcher.add(\"\", pattern)\n",
    "\n",
    "# for doc in docs:\n",
    "# do matches and save the text in a list\n",
    "\n",
    "# count the number of times the same expression appears in the list (hint: remember the dictionary solution...)\n",
    "# take the top 10 of the counter\n",
    "# the result will be a list of tuples of the form (count, expression)\n",
    "\n",
    "# most_common_pobj = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f090a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dd2eee9295599d16",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(most_common_pobj)== list, \"the output is not a list\"\n",
    "assert len(most_common_pobj) == 10, \"It should be the top 10!\"\n",
    "\n",
    "assert _hash(most_common_pobj) == 'eb1416f528dcaa1f08b3c1f3460e2079f8baedb953f8ca810347adbf2d34e52a', 'The top ten list is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f44647",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b9f1ddf5f7c7ec1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This time the counts are higher and might be more interesting for a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568accf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53fa28fe45c63cfb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_common_pobj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98e7c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad6aa5c72ba1a3e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.e) Verbs with direct objects\n",
    "As the last point, you decide to look at verbs with direct objects. These should indicate actions taken towards something or someone. This one can be done without a Matcher.\n",
    "\n",
    "Search for verbs with direct objects which are not pronouns. This time it's a bit trickier - you need to look at the [parse tree](https://spacy.io/usage/linguistic-features#navigating) because the object does not necessarily come right after the verb. Lemmatize both the verb and the object and count the occurences of the lemmatized verb and direct object separated by a space, like this : 'verb_lemma dobj_lemma'. Don't forget to exclude objects that are pronouns.\n",
    "\n",
    "Again, output the 10 most common as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d62ce",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9be3562d21acdf58",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hint: you need to use the parse tree and the dependency and POS labels\n",
    "\n",
    "# for doc in docs:\n",
    "# do matches and save the text in a list\n",
    "\n",
    "# count the number of times the same expression appears in the list (hint: remember the dictionary solution...)\n",
    "# take the top 10 of the counter\n",
    "# the result will be a list of tuples of the form (count, expression)\n",
    "\n",
    "# most_common_dobj = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8593b48",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-aa931f62fa91ab35",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(most_common_dobj)== list, \"the output is not a list\"\n",
    "assert len(most_common_dobj) == 10, \"It should be the top 10!\"\n",
    "\n",
    "assert _hash(most_common_dobj) == 'eb869c73f4e54f102e4b7e0b24fe2b058ff941d01522520ed61d1327ffe6b891', 'The top ten list is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85479cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-849a37fbcedc4049",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Not so many occurencies, but the whole list could be used with a vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b81e2e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-218910800564bd8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_common_dobj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8244da23",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59afa572e05bc308",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q3. Feature Unions\n",
    "\n",
    "We're going to create a few more numerical features here, then use them in a feature union pipeline and see if the baseline improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca16469",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-692f18d2b61996fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q3.a) Adding Extra Features\n",
    "\n",
    "There are a few more simple features that we can extract from the dataset to try to enrich our model. Let's add to our dataframe the following features: **number of words in the news article**, **character length of the news article**,  **average word length**, and **average sentence length**. (Remember that we already have the number of adverbs and adjectives.)\n",
    "\n",
    "Use the SpaCy processed `Doc`s for calculating the sentence length and don't forget to exclude punctuation. Use the tokenized text in `df_processed` for everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb93e53",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6910256a13fa82fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# df_processed[\"nb_words\"] = ...\n",
    "# df_processed[\"doc_length\"] = ...\n",
    "# df_processed[\"avg_word_length\"] = ...\n",
    "# df_processed[\"avg_sentence_length\"] = ...\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5b354",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-31106be84628e3c9",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert df_processed.shape == (5000, 7), \"Something wrong about the shape, do you have all columns/rows?\"\n",
    "assert \"nb_words\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"doc_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"avg_word_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"avg_sentence_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "\n",
    "assert np.sum(df_processed[\"nb_words\"]) == 1963935, \"Something wrong with the nb_words column.\"\n",
    "assert np.sum(df_processed[\"doc_length\"]) == 14636737, \"Something wrong with the doc_length column.\"\n",
    "np.testing.assert_almost_equal(np.sum(df_processed[\"avg_word_length\"]), 32100.0, decimal=1, err_msg='Something is wrong with the avg_word_length column.')\n",
    "np.testing.assert_almost_equal(np.sum(df_processed[\"avg_sentence_length\"]), 116678.1, decimal=1, err_msg='Something is wrong with the avg_sentence_length column.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1e3de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b12d83fd4d036457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Q3.b) Feature Pipelines\n",
    "\n",
    "Let's create a processing _Pipeline_ for every new feature and join them all in a _Feature Union_. For the textual features use the usual _TfidfVectorizer_ with default parameters and for any numerical feature use a _Standard Scaler_. Afterwards, join the features pipelines using a _Feature Union_.\n",
    "\n",
    "Use the following Selector classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935c549",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61e270fd37131ec7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a column from the dataframe to perform additional transformations on\n",
    "    \"\"\" \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "    \n",
    "class NumberSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e0bf1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a5c8a9abba2b5e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# text_pipe = Pipeline([...])\n",
    "# nb_adj_adv_pipe = Pipeline([...])\n",
    "# nb_words_pipe = Pipeline([...])\n",
    "# doc_length_pipe = Pipeline([...])\n",
    "# avg_word_length_pipe = Pipeline([...])\n",
    "# avg_sentence_length_pipe = Pipeline([...])\n",
    "# feats = FeatureUnion(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402bc439",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-98151e411b73b1f5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(feats, FeatureUnion)\n",
    "assert len(feats.transformer_list) == 6, \"Are you creating 6 pipelines? One for each feature?\"\n",
    "for pipe in feats.transformer_list:\n",
    "    \n",
    "    selector = pipe[1][0]\n",
    "    if not (isinstance(selector, TextSelector) or isinstance(selector, NumberSelector)):\n",
    "        raise AssertionError(\"pipeline is wrong, the Selectors should come first.\")\n",
    "        \n",
    "    feature_builder = pipe[1][1]\n",
    "    if not (isinstance(feature_builder, TfidfVectorizer) or isinstance(feature_builder, StandardScaler)):\n",
    "        raise AssertionError(\"pipeline is wrong, the second thing to come should be the Tfidf or the Scaler.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb7c8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8c92614d69d640a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Q3.c) Feature Union\n",
    "Now let's build our function to use the newly created _Feature Union_ and calculate its performance!\n",
    "\n",
    "Create a function that will apply the improved pipeline to the provided train data, make a prediction and calculate its accuracy. The pipeline should consist of the feature union we created in Q3.b and a RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431c958",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb15cc415e6737d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def improved_pipeline(feats, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Creates a pipeline with the provided feature union and a Random Forest classifier.\n",
    "    Fits the pipeline to the train data and makes a prediction with the test data.\n",
    "    Outputs the fitted pipeline and the accuracy of the prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # pipe = (...)\n",
    "    # pipe.fit(...)\n",
    "    # (...)\n",
    "    # acc = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return pipe, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da0430",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-873e3b1b663a7b12",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Y = df_processed[\"label\"]\n",
    "X = df_processed.drop(columns=\"label\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "pipeline_model, pipeline_acc = improved_pipeline(feats, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# asserts\n",
    "assert isinstance(pipeline_model, Pipeline)\n",
    "assert _hash(pipeline_model[0]) == '933e9e022884461f96b8dcbda7872290b8fd44f4003fa618ab5ff8d2a952c247', \"The first part of the\\\n",
    "Pipeline is incorrect.\"\n",
    "assert _hash(pipeline_model[1]) == 'e5fd22909dcc06f7c81407ee302879e41a75675ddfd55fa1ec640ae68a3338d8', \"The second part of the\\\n",
    "Pipeline is incorrect.\"\n",
    "assert np.allclose(pipeline_acc, 0.913, rtol=1e-1), \"Something wrong with the accuracy score. Use the default parameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e0258",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec9f67cb4f0f5b43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With this more complex approach we have achieved basically the same performance as our baseline. This might mean a lot of things: our features might have no real revelance to the model (which you can check with feature importances) or we have achieved a plateau and can't improve the score with this technique. \n",
    "\n",
    "Nevertheless it is a good score for this problem and dataset. Regardless the score, you have learnt a lot about _SpaCy_, _Feature Union_ and also learnt that the sky is the limit when creating features. Anything can be a feature really - now good features are a totally different thing that might need more research and validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
